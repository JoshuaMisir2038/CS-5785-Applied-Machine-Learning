{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data points: 7613\n",
      "Number of test data points: 3263\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import re\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "########## 1. Download the Data ##########\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# 1A How many training and test data points are there?\n",
    "\n",
    "print(f\"Number of training data points: {train.shape[0]}\")\n",
    "print(f\"Number of test data points: {test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the percentage of real disasters in the training set is: 42.97 % \n",
      "the percentage of tweets that are not actual disasters in the training set is: 57.03%\n"
     ]
    }
   ],
   "source": [
    "# 1A what percentage of the training tweets are real disasters and are not?\n",
    "\n",
    "percent_real = (train[\"target\"].sum()/ train.shape[0]) * 100\n",
    "print(f\"the percentage of real disasters in the training set is: {round(percent_real, 2)} % \")\n",
    "print(f\"the percentage of tweets that are not actual disasters in the training set is: {round(100 - percent_real,2)}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b Split the training data set into a training set and development set 70/30\n",
    "\n",
    "training_split = train.sample(frac = .70)\n",
    "development_split = train.drop(training_split.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1C - remove all Twitter tags\n",
    "\n",
    "\n",
    "training_split['text'] = training_split['text'].replace(r'@[A-Za-z0-9]+', '', regex=True)\n",
    "development_split['text'] = development_split['text'].replace(r'@[A-Za-z0-9]+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1C Preprocess the data\n",
    "\n",
    "# 1C - overting all letters to lowercase\n",
    "training_split['text'] = training_split.text.apply(lambda x: x.lower())\n",
    "development_split['text'] = development_split.text.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1C -  remove all special characters and punctuation\n",
    "\n",
    "training_split['text'] = training_split['text'].replace(r'[^\\w\\s]|_', '', regex=True)\n",
    "development_split['text'] = development_split['text'].replace(r'[^\\w\\s]|_', '', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1C - remove all URLS (e.g. https and www.)\n",
    "\n",
    "training_split['text'] = training_split['text'].replace(r'http\\S+|www.\\.\\S+', '', regex=True)\n",
    "development_split['text'] = development_split['text'].replace(r'http\\S+|www.\\.\\S+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1C - strip all the stop words (e.g. the, and, or)\n",
    "\n",
    "\n",
    "training_split['text'] = training_split['text'].replace(r'(\\s*\\b(?:a|an|and|are|as|at|be|but|by|for|if|in|into|is|it|no|not|of|on|or|such|that|the|their|then|there|these|they|this|to|was|will|with|my|oh|i|were|werent|was|wasnt|do|does))\\b', '', regex=True)\n",
    "development_split['text'] = development_split['text'].replace(r'(\\s*\\b(?:a|an|and|are|as|at|be|but|by|for|if|in|into|is|it|no|not|of|on|or|such|that|the|their|then|there|these|they|this|to|was|will|with|my|oh|i|were|werent|was|wasnt|do|does))\\b', '', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/joshuamisir/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/joshuamisir/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/joshuamisir/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 1C - Lemmatize all the words\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import tokenize, stem, corpus\n",
    "\n",
    "\n",
    "lemmatizer = stem.WordNetLemmatizer()\n",
    "\n",
    "# Get the POS Tag for lemmatization (reference: https://medium.com/@yashj302/lemmatization-f134b3089429 )\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "    tag_dict = {'j': corpus.wordnet.ADJ,\n",
    "                'n': corpus.wordnet.NOUN,\n",
    "                'v': corpus.wordnet.VERB,\n",
    "                'r': corpus.wordnet.ADV}\n",
    "    return tag_dict.get(tag, corpus.wordnet.NOUN)\n",
    "\n",
    "# apply lemmatizer on the text column\n",
    "training_split['text'] = training_split.text.apply(lambda x: ' '.join([lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in tokenize.word_tokenize(x)]))\n",
    "development_split['text'] = development_split.text.apply(lambda x: ' '.join([lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in tokenize.word_tokenize(x)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1d) Bag of words model & determining optimal M\n",
    "\n",
    "f1_trainlist = []\n",
    "f1_devlist = []\n",
    "\n",
    "# threshold for bag of words \n",
    "for M in range(12, 30):\n",
    "\n",
    "# Perform CountVectorizer on text. This creates three numbers:\n",
    "# 1) list of rows which represent tweets\n",
    "# 2) a feature index which represents a distinct word within that tweet \n",
    "# 3) a count which represents how many times that word has been used\n",
    "    \n",
    "    count_vect = CountVectorizer(binary=True, min_df=M)\n",
    "    X_train = count_vect.fit_transform(training_split['text'])\n",
    "\n",
    "    # need to make dev set transformed to fit X_train size\n",
    "    X_dev = count_vect.transform(development_split['text'])\n",
    "\n",
    "    # create logistic model\n",
    "    logreg = LogisticRegression(multi_class='auto', penalty='none', max_iter=3000)\n",
    "    logreg.fit(X_train, training_split['target'].values)\n",
    "\n",
    "    # calculate F1 score\n",
    "    f1_train = f1_score(training_split['target'].values, logreg.predict(X_train))\n",
    "    f1_dev = f1_score(development_split['target'].values, logreg.predict(X_dev))\n",
    "\n",
    "    f1_trainlist.append(f1_train)\n",
    "    f1_devlist.append(f1_dev)\n",
    "\n",
    "\n",
    "count_vect1 = CountVectorizer(binary=True, min_df=12)\n",
    "X_train1 = count_vect1.fit_transform(training_split['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "907\n"
     ]
    }
   ],
   "source": [
    "print(len(count_vect1.vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot to determine optimal M\n",
    "plt.plot(range(12, 30), f1_trainlist, label=\"Training Set\")\n",
    "plt.plot(range(12, 30), f1_devlist, label=\"Dev Set\")\n",
    "plt.xlabel(\"M Threshold\")\n",
    "plt.ylabel(\"F1-Score\")\n",
    "leg = plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# F1-Dev Score is maxed at ~M = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 13\n",
    "\n",
    "# Perform CountVectorizer on text\n",
    "count_vect = CountVectorizer(binary=True, min_df=M)\n",
    "X_train = count_vect.fit_transform(training_split['text'])\n",
    "\n",
    "# need to make dev set transformed to fit X_train size\n",
    "X_dev = count_vect.transform(development_split['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set with no regularization terms F1-Score is: 0.8299258593574478\n",
      "Development set with no regularization terms F1-Score is: 0.7257203842049094\n"
     ]
    }
   ],
   "source": [
    "# 1e) i - LogisticRegression with no regularization. Penalty = none\n",
    "\n",
    "# The F1 score on the training set is higher than the F1 score on the development set which contains unseen data\n",
    "# This is an example of  overfitting. \n",
    "# Our model is too strongly fitted to the training data when there is no regularization in the Logistic Regression\n",
    "# This is due to the fact that there is no regularization factor in this logistic regression.\n",
    "# The lack of regularization makes this model too sensative to the \"noise\" (data points that dont truly represent the data) \n",
    "# in the data set\n",
    "\n",
    "logreg = LogisticRegression(multi_class='ovr', penalty='none', max_iter=2500)\n",
    "logreg.fit(X_train, training_split['target'].values)\n",
    "\n",
    "# calculate F1 score\n",
    "f1_train = f1_score(training_split['target'].values, logreg.predict(X_train))\n",
    "f1_dev = f1_score(development_split['target'].values, logreg.predict(X_dev))\n",
    "\n",
    "\n",
    "# print F1 values out\n",
    "#print(f\"Training set with no regularization terms F1-Score is: {round(f1_train,3)}\")\n",
    "#print(f\"Development set with no regularization terms F1-Score is: {round(f1_dev,3)}\")\n",
    "\n",
    "print(f\"Training set with no regularization terms F1-Score is: {f1_train}\")\n",
    "print(f\"Development set with no regularization terms F1-Score is: {f1_dev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set with L2 regularization F1-Score is 0.8034306907742236\n",
      "Development set with L2 regularization F1-Score is 0.7252867285636264\n"
     ]
    }
   ],
   "source": [
    "# 1e) ii - LogisticRegression with L1 regularization. \n",
    "\n",
    "\n",
    "logreg1 = LogisticRegression(penalty='l1', max_iter=2500, solver='liblinear')\n",
    "logreg1.fit(X_train, training_split['target'].values)\n",
    "\n",
    "# calculate F1 score\n",
    "f1_train_reg1 = f1_score(training_split['target'].values, logreg1.predict(X_train))\n",
    "f1_dev_reg1 = f1_score(development_split['target'].values, logreg1.predict(X_dev))\n",
    "\n",
    "\n",
    "#print F1 values out\n",
    "print(f\"Training set with L2 regularization F1-Score is {f1_train_reg1}\")\n",
    "print(f\"Development set with L2 regularization F1-Score is {f1_dev_reg1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set with L2 regularization F1-Score is 0.8091497227356746\n",
      "Development set with L2 regularization F1-Score is 0.7351174221736756\n"
     ]
    }
   ],
   "source": [
    "# 1e) iii - LogisticRegression with L2 regularization. \n",
    "\n",
    "\n",
    "logreg2 = LogisticRegression(penalty='l2', max_iter=2500)\n",
    "logreg2.fit(X_train, training_split['target'].values)\n",
    "\n",
    "# calculate F1 score\n",
    "f1_train_reg2 = f1_score(training_split['target'].values, logreg2.predict(X_train))\n",
    "f1_dev_reg2 = f1_score(development_split['target'].values, logreg2.predict(X_dev))\n",
    "\n",
    "\n",
    "#print F1 values out\n",
    "print(f\"Training set with L2 regularization F1-Score is {f1_train_reg2}\")\n",
    "print(f\"Development set with L2 regularization F1-Score is {f1_dev_reg2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1e) iv - Which one of the three classifiers performed the best on your training and development set?\n",
    "\n",
    "# The best performing model on both the training and development set was the \n",
    "# Logistic Regression with L2 regularization. \n",
    "# The superior outcomes for this logistic regression can best be explained by the fact that L2\n",
    "# squares the magnitude of the coeffecicients whereas the L1 regularization takes the absolute value\n",
    "# of the coeffecients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['derailment', 'drought', 'earthquake', 'hiroshima', 'massacre', 'typhoon', 'wildfire']\n"
     ]
    }
   ],
   "source": [
    "# 1e) v.  Inspect weight vector of classifier with L1 regularization\n",
    "\n",
    "# retrieve index from coefficiets that are greater than a value\n",
    "impt_index = [i for i,v in enumerate(logreg2.coef_.tolist()[0]) if v > 2]\n",
    "\n",
    "# map the index to the actual words\n",
    "impt_words = [count_vect.get_feature_names()[i] for i in impt_index]\n",
    "\n",
    "# The most important words in determining if the Tweet was a real disaster\n",
    "print(impt_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.56821167 0.43178833]\n"
     ]
    }
   ],
   "source": [
    "#1f) Bernoulli Naive Bayes\n",
    "\n",
    "# laplace smoothing parameter\n",
    "alpha = .01\n",
    "\n",
    "# create array of target values\n",
    "y_train = training_split['target'].values\n",
    "\n",
    "# convert Xtrain to array for manipulation\n",
    "X_train = X_train.toarray()\n",
    "n = X_train.shape[0]    # size of the data set\n",
    "d = X_train.shape[1]    # number of features in the data set\n",
    "K = 2                   #number of classes (Real/not real disasters)\n",
    "\n",
    "# initialize shapes of parameters\n",
    "\n",
    "#An array with two rows (classes, K) and 798 cols (features, d)\n",
    "psis = np.zeros([K,d]) \n",
    "\n",
    "# An array with K (2) number of rows\n",
    "phis = np.zeros([K])\n",
    "\n",
    "\n",
    "# calculate the parameters, and apply alpha\n",
    "for k in range(K):\n",
    "    X_k = X_train[y_train == k]\n",
    "    psis[k] = (np.sum(X_k, axis=0) + alpha)/(X_k.shape[0] + 2*alpha)\n",
    "    phis[k] = X_k.shape[0] / float(n)\n",
    "    \n",
    "print(phis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8151623193844999"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nb_predictions(x, psis, phis):\n",
    "    \"\"\"This returns class assignments and scores under the NB model.\n",
    "    \n",
    "    We compute \\arg\\max_y p(y|x) as \\arg\\max_y p(x|y)p(y)\n",
    "    \"\"\"\n",
    "    # adjust shapes\n",
    "    n, d = x.shape\n",
    "    x = np.reshape(x, (1, n, d))\n",
    "    psis = np.reshape(psis, (K, 1, d))\n",
    "    \n",
    "    # clip probabilities to avoid log(0)\n",
    "    psis = psis.clip(1e-14, 1-1e-14)\n",
    "    \n",
    "    # compute log-probabilities\n",
    "    logpy = np.log(phis).reshape([K,1])\n",
    "    logpxy = x * np.log(psis) + (1-x) * np.log(1-psis)\n",
    "    logpyx = logpxy.sum(axis=2) + logpy\n",
    "\n",
    "    return logpyx.argmax(axis=0).flatten(), logpyx.reshape([K,n])\n",
    "\n",
    "idx_train, logpyx_train = nb_predictions(X_train, psis, phis)\n",
    "\n",
    "\n",
    "(idx_train==y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7915936952714536"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# calculate the predictions for dev set\n",
    "\n",
    "y_dev = development_split['target'].values\n",
    "\n",
    "# convert Xtrain to array for manipulation\n",
    "X_dev = X_dev.toarray()\n",
    "\n",
    "idx_dev, logpyx_train = nb_predictions(X_dev, psis, phis)\n",
    "\n",
    "# calculate the accurary\n",
    "(idx_dev==y_dev).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which model performed the best in predicting whether a tweet is of a real disaster or\n",
    "# not? Include your performance metric in your response. \n",
    "# Comment on the pros and cons of using generative vs discriminative models. \n",
    "# • Think about the assumptions that Naive Bayes makes. \n",
    "# How are the assumptions different from logistic regressions? Discuss whether it’s valid and efficient to use Bernoulli\n",
    "# Naive Bayes classifier for natural language texts.\n",
    "\n",
    "\n",
    "\n",
    "# 1g) The Naive Bayes model produced a better result with an accuracy rate of 80% vs. Logistic regression, which had \n",
    "# a F1-score of 72%. \n",
    "# However, NB method assumes that all the words are not correlated, which isn't true. This results in the probabilities\n",
    "# being overconfidenti n its predictions. It's valid to use Bernoulli NB classifier if the feature space is small, as it'll\n",
    "# reduce the number of instances of dependent features. Otherwise, if the feature space is large, Logistic regression is better\n",
    "\n",
    "# This can be measuered in practice. For example if you have a classification problem where you are attempting to\n",
    "# classify what city a news article is being published for, either Boston or New York. Lets assume Boston \n",
    "# and New York are refenced equally throughout the text in their respective classes. \"New York\" will contribute\n",
    "# twice the weight as one instance of \"Boston\". The summed contribution of the classification weights may be larger\n",
    "# for one class than another which will cause the NB model to favor one class over the other. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# One issue with Naive Bayes is that if there is unequal representativeness of class representations in the training data \n",
    "# within a binary classification problem, there will be a bias toward that over-represented class. This will degrade accuracy \n",
    "\n",
    "# Another assumption is that Naive Bayes assumes that words are idependent and identically distributed which \n",
    "# is a faulty assumption and is not the case in practice. \n",
    "# Certain pairs of words within sentences have a high likelihood of being seen together\n",
    "# which reduces the strength of the identical, independent distrubtion Naive Bayes makes. \n",
    "\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild dataset and re-train on NB with 2-grams\n",
    "\n",
    "h_train = pd.read_csv('train.csv')\n",
    "h_test = pd.read_csv('test.csv')\n",
    "\n",
    "# Pre-processing the data\n",
    "\n",
    "# remove all lowercase from the tweet\n",
    "h_train['text'] = h_train.text.apply(lambda x: x.lower())\n",
    "h_test['text'] = h_test.text.apply(lambda x: x.lower())\n",
    "\n",
    "# remove all Twitter tags\n",
    "h_train['text'] = h_train['text'].replace(r'@[A-Za-z0-9]+', '', regex=True)\n",
    "h_test['text'] = h_test['text'].replace(r'@[A-Za-z0-9]+', '', regex=True)\n",
    "\n",
    "# remove all special characters, including punctuation\n",
    "h_train['text'] = h_train['text'].replace(r'[^\\w\\s]|_', '', regex=True)\n",
    "h_test['text'] = h_test['text'].replace(r'[^\\w\\s]|_', '', regex=True)\n",
    "\n",
    "# remove all URLs\n",
    "h_train['text'] = h_train['text'].replace(r'http\\S+|www.\\.\\S+', '', regex=True)\n",
    "h_test['text'] = h_test['text'].replace(r'http\\S+|www.\\.\\S+', '', regex=True)\n",
    "\n",
    "# strip all the stop words (e.g. the, and, or)\n",
    "h_train['text'] = h_train['text'].replace(r'(\\s*\\b(?:a|an|and|are|as|at|be|but|by|for|if|in|into|is|it|no|not|of|on|or|such|that|the|their|then|there|these|they|this|to|was|will|with|my|oh|i|were|werent|was|wasnt|do|does))\\b', '', regex=True)\n",
    "h_test['text'] = h_test['text'].replace(r'(\\s*\\b(?:a|an|and|are|as|at|be|but|by|for|if|in|into|is|it|no|not|of|on|or|such|that|the|their|then|there|these|they|this|to|was|will|with|my|oh|i|were|werent|was|wasnt|do|does))\\b', '', regex=True)\n",
    "\n",
    "# lemmatize the words\n",
    "h_train['text'] = h_train.text.apply(lambda x: ' '.join([lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in tokenize.word_tokenize(x)]))\n",
    "h_test['text'] = h_test.text.apply(lambda x: ' '.join([lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in tokenize.word_tokenize(x)]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8188624720872192"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1h) countvector with n-grams = 1,2\n",
    "\n",
    "count_vect = CountVectorizer(binary=True,ngram_range=(1,2), min_df=12)\n",
    "X_train = count_vect.fit_transform(h_train['text'])\n",
    "X_test = count_vect.transform(h_test['text'])\n",
    "\n",
    "# NB on the training data\n",
    "\n",
    "# create array of target values\n",
    "Y_train = h_train['target'].values\n",
    "\n",
    "# convert Xtrain to array for manipulation\n",
    "X_train = X_train.toarray()\n",
    "n = X_train.shape[0]  # size of the data set (number of tweets)\n",
    "d = X_train.shape[1]  # number of features in the data set\n",
    "K = 2                 # number of classes we are classification classes\n",
    "\n",
    "# initialize shapes of parameters\n",
    "psis = np.zeros([K,d])\n",
    "phis = np.zeros([K])\n",
    "\n",
    "# calculate the parameters, and apply alpha\n",
    "for k in range(K):\n",
    "    X_k = X_train[Y_train == k]\n",
    "    psis[k] = (np.sum(X_k, axis=0) + alpha)/(X_k.shape[0] + 2*alpha)\n",
    "    phis[k] = X_k.shape[0] / float(n)\n",
    "\n",
    "idx_train, logpyx_train = nb_predictions(X_train, psis, phis)\n",
    "\n",
    "# calculate the accurary\n",
    "(idx_train==Y_train).mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index (7593) out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-ecaee38ddf05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# calculate the parameters, and apply alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mX_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mY_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mpsis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mphis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \"\"\"\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Dispatch to specialized methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINT_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m_validate_indices\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0mrow\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_asindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misintlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m_asindices\u001b[0;34m(self, idx, length)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mmax_indx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax_indx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index (%d) out of range'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmax_indx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mmin_indx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index (7593) out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "count_vect = CountVectorizer(binary=True,ngram_range=(1,2), min_df=12)\n",
    "X_train = count_vect.fit_transform(h_train['text'])\n",
    "X_test = count_vect.transform(h_test['text'])\n",
    "\n",
    "# NB on the training data\n",
    "\n",
    "# create array of target values\n",
    "Y_train = h_train['target'].values\n",
    "\n",
    "# convert Xtrain to array for manipulation\n",
    "X_train = X_train.toarray()\n",
    "n = X_train.shape[0]  # size of the data set (number of tweets)\n",
    "d = X_train.shape[1]  # number of features in the data set\n",
    "K = 2                 # number of classes we are classification classes\n",
    "\n",
    "# initialize shapes of parameters\n",
    "psis = np.zeros([K,d])\n",
    "phis = np.zeros([K])\n",
    "\n",
    "# calculate the parameters, and apply alpha\n",
    "for k in range(K):\n",
    "    X_k = X_test[Y_train == k]\n",
    "    psis[k] = (np.sum(X_k, axis=0) + alpha)/(X_k.shape[0] + 2*alpha)\n",
    "    phis[k] = X_k.shape[0] / float(n)\n",
    "\n",
    "idx_train, logpyx_train = nb_predictions(X_test, psis, phis)\n",
    "\n",
    "# calculate the accurary\n",
    "(idx_train==Y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(count_vect.vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Xtrain to array for manipulation\n",
    "X_test = X_test.toarray()\n",
    "\n",
    "idx_test, logpyx_test= nb_predictions(X_test, psis, phis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create submission\n",
    "h_test[\"target\"] = idx_test\n",
    "submission = h_test[[\"id\",\"target\"]]\n",
    "submission.to_csv('submission_disastertweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
